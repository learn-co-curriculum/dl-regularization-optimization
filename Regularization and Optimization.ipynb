{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How optimize and regularize your networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hyperparameters and iterative deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are many hyperparameters you can tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- number of hidden units\n",
    "- number of layers\n",
    "- learning rate alpha\n",
    "- activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training/hold-out/test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous labs, we have been inconsistent about what to do regarding testing our models when trying to optimize them. For our santa examples, we have been using a training and a test set, but for the two circles example, we were both training and evaluation on the same data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's formalize how we'll proceed doing this. The fact that there are so many hyperparameters to tune calls for a formalized and unbiased approach. You've seen before that you want to use a training and a test set, because it is \"unfair\" to evaluate your model on the same data you used to select a certain model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short, we'll use 3 sets when running, selecting and validating a model:\n",
    "- You train algorithms on the training set\n",
    "- You'll use a validation set to decide which one will be your final model after parameter tuning\n",
    "- After having chosen the final model (and having evaluated long enough), you'll use the test set to get an unbiased estimate of the classification performance (or whatever your evaluation metric will be)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With big data, your dev and test sets don't necessarily need to be 20-30% of all the data. You can choose test and hold-out sets that are of size 1-5%. eg. 96% train, 2% hold-out, 2% test set. \n",
    "\n",
    "VERY IMPORTANT to make sure holdout and test sample come from the same distribution: eg. same resolution of santa pictures.\n",
    "\n",
    "What we did before is actually use the holdout set as a test set as well! actually our \"test set\" was a holdout set. You're basically overfitting to the test set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bias and variance in deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Our circles example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In classical models and machine learning, often one talks about a \"bias-variance trade-off\". We'll discuss these concepts here, and tell how deep learning is slightly different and a trade-off isn't always present!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias = underfitting\n",
    "\n",
    "high variance = overfitting\n",
    "\n",
    "good fit --> somewhere in between"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "examples: let's take another look at our two circles data, the data looked like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](figures/example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we fit a logistic regression model to the data here. We got something that looked like the picture below. The model didn't do a particularly good job at discriminating between the yellow and purple dots. We would say this is a model with a **high bias**, the model is **underfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](figures/underfitting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using a neural network, what we reached in the end was a pretty good decision boundary, a circle discriminating between the yellow and purple dots:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](figures/good.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the other end of the spectrum, we might experience **overfitting**, where we create a circle which is super sensitive to small deviations of the colored dots. An example below. We also call this a model with **high variance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](figures/overfitting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 The santa example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tr>\n",
    "<td> <img src=\"figures/s_4.jpg\" alt=\"Drawing\" style=\"height: 220px;\"/> </td>\n",
    "<td> <img src=\"figures/ns_1.jpg\" alt=\"Drawing\" style=\"height: 220px;\"/> </td>\n",
    "</tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|       | high variance | high bias   | high variance & bias | low variance and bias |\n",
    "|-------|---------------|-------------|----------------------|-----------------------|\n",
    "|train set error|   12% | 26%         | 26%                  | 12%                  |\n",
    "|validation set error|   25% | 28%         | 40%                   | 13%                   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that our best model can get to a validation set accuracy of 87%. Note that \"high\" and \"low\" are relative! Also, in deep learning there is less of a bias variance trade-off!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Rules of thumb regarding bias / variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| High Bias? (training perf) | high variance? (validation perf)  |\n",
    "|---------------|-------------|\n",
    "| Use a bigger network|    More data     |\n",
    "| Train longer | Regularization   |\n",
    "| Look for other existing NN architextures |Look for other existing NN architextures |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use when overfitting happens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 L1 and L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 In logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look back at the logistic regression-example. with lambda a regularization parameter (another hyperparameter you have to tune).\n",
    "\n",
    "$$ J (w,b) = \\dfrac{1}{m} \\sum^m_{i=1}\\mathcal{L}(\\hat y^{(i)}, y^{(i)})+ \\dfrac{\\lambda}{2m}||w||_2^2$$\n",
    "\n",
    "$$||w||_2^2 = \\sum^{n_x}_{j=1}w_j^2= w^Tw$$\n",
    "\n",
    "This is called L2-regularization. You can also add a regularization term for $b$, but $b$ is just one parameter. L2-regularization is the most common type of regularization.\n",
    "\n",
    "L1-regularization is where you just add a term:\n",
    "\n",
    "$$ \\dfrac{\\lambda}{m}||w||_1$$ (could also be 2 in the denominator)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 In a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ J (w^{[1]},b^{[1]},...,w^{[L]},b^{[L]}) = \\dfrac{1}{m} \\sum^m_{i=1}\\mathcal{L}(\\hat y^{(i)}, y^{(i)})+ \\dfrac{\\lambda}{2m}\\sum^L_{l=1}||w^{[l]}||^2$$\n",
    "\n",
    "$$||w^{[l]}||^2 = \\sum^{n^{[l-1]}}_{i=1} \\sum^{n^{[l]}}_{j=1} (w_{ij}^{[l]})^2$$\n",
    "\n",
    "this matrix norm is called the \"Frobenius norm\", also referred to as $||w^{[l]}||^2 _F$\n",
    "\n",
    "\n",
    "How does backpropagation change now?\n",
    "whichever expression you have from the backpropagation, and add $\\dfrac{\\lambda}{m} w^{[l]}$.\n",
    "So,\n",
    "\n",
    "$$dw^{[l]} = \\text{[backpropagation derivatives] }+ $\\dfrac{\\lambda}{m} w^{[l]}$$ \n",
    "\n",
    "Afterwards, $w^{[l]}$ is updated again as $w^{[l]}:= w^{[l]} - \\alpha dw^{[l]} $\n",
    "\n",
    "L2-regularization is called weight decay, because regularization will make your load smaller:\n",
    "\n",
    "$$w^{[l]}:= w^{[l]} - \\alpha \\bigr( \\text{[backpropagation derivatives] }+ \\dfrac{\\lambda}{m} w^{[l]}\\bigr)$$\n",
    "\n",
    "$$w^{[l]}:= w^{[l]} - \\dfrac{\\alpha\\lambda}{m}w^{[l]} - \\alpha \\text{[backpropagation derivatives]}$$\n",
    "\n",
    "hence your weights will become smaller by a factor $\\bigr(1- \\dfrac{\\alpha\\lambda}{m}\\bigr)$.\n",
    "\n",
    "Intuition for regularization: the weight matrices will be penalized from being too large. Actually, the network will be forced to almost be simplified.\n",
    "Also: eg, tanh function, if $w$ small, the activation function will be mostly operating in the linear region and not \"explode\" as easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 dropout regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each node, drop a coin and drop them out (you can also alter the dropout probability to be different from 0.5).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement dropout for layer l. Let's implement a dropout vector for layer l, denoted by $dl$:\n",
    "\n",
    "```\n",
    "Specify keep_prob = 0.8\n",
    "dl = np.random.rand(al.shape[0], al.shape[1]) < keep_prob\n",
    "```\n",
    "Activations:\n",
    "\n",
    "```\n",
    "al = np.multiply(al,dl)\n",
    "```\n",
    "\n",
    "--> there is a 20% chance that each of the elements is 0, you'll zero out the corresponding element. As a last step, you need to divide a3 by `keep_prob`, in order to not reduce the expected value of $z^{[l+1]}$\n",
    "\n",
    "```\n",
    "a1 /= keep_prob \n",
    "```\n",
    "\n",
    "In different iterations through the training set, different nodes will be zeroed out!\n",
    "\n",
    "When making predictions, don't do dropout!\n",
    "\n",
    "Why does dropout regularization work? The idea is that we train the network in a way that in cannot rely on any specific feature, so the weights are spread out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Other types of regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation\n",
    "\n",
    "early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. normalized inputs will speed up training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- subtracting the mean\n",
    "- normalize by dividing the variances\n",
    "- learning can be slow when inputs are unnormalized because of different scales.\n",
    "- Example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~LoreDirick/12.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "w1 = np.arange(-5, 5, 0.1)\n",
    "w2 = np.arange(-5, 5, 0.1)\n",
    "w1,w2 = np.meshgrid(w1, w2)\n",
    "J = w1**2+ w2**2\n",
    "\n",
    "surface = go.Surface(x=w1, y=w2, z=J, colorscale='Viridis')\n",
    "data = [surface]\n",
    "layout = go.Layout(\n",
    "title='Normalized inputs',\n",
    "    scene=dict(\n",
    "        xaxis=dict(\n",
    "            gridcolor='rgb(255, 255, 255)',\n",
    "            zerolinecolor='rgb(255, 255, 255)',\n",
    "            showbackground=True,\n",
    "            backgroundcolor='rgb(230, 230,230)'\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            gridcolor='rgb(255, 255, 255)',\n",
    "            zerolinecolor='rgb(255, 255, 255)',\n",
    "            showbackground=True,\n",
    "            backgroundcolor='rgb(230, 230,230)'\n",
    "        ),\n",
    "        zaxis=dict(\n",
    "            gridcolor='rgb(255, 255, 255)',\n",
    "            zerolinecolor='rgb(255, 255, 255)',\n",
    "            showbackground=True,\n",
    "            backgroundcolor='rgb(230, 230,230)'\n",
    "        )\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='inputs_normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~LoreDirick/8.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "w1 = np.arange(-50, 50, 0.1)\n",
    "w2 = np.arange(-5, 5, 0.1)\n",
    "w1,w2 = np.meshgrid(w1, w2)\n",
    "J = w1**2+ w2**2\n",
    "\n",
    "surface = go.Surface(x=w1, y=w2, z=J, colorscale='Viridis')\n",
    "data = [surface]\n",
    "layout = go.Layout(\n",
    "title='Unnormalized inputs',\n",
    "    scene=dict(\n",
    "        xaxis=dict(\n",
    "            gridcolor='rgb(255, 255, 255)',\n",
    "            zerolinecolor='rgb(255, 255, 255)',\n",
    "            showbackground=True,\n",
    "            backgroundcolor='rgb(230, 230,230)'\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            gridcolor='rgb(255, 255, 255)',\n",
    "            zerolinecolor='rgb(255, 255, 255)',\n",
    "            showbackground=True,\n",
    "            backgroundcolor='rgb(230, 230,230)'\n",
    "        ),\n",
    "        zaxis=dict(\n",
    "            gridcolor='rgb(255, 255, 255)',\n",
    "            zerolinecolor='rgb(255, 255, 255)',\n",
    "            showbackground=True,\n",
    "            backgroundcolor='rgb(230, 230,230)'\n",
    "        )\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='inputs_unnormalized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing or exploding gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.coursera.org/learn/deep-neural-network/lecture/lXv6U/normalizing-inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
